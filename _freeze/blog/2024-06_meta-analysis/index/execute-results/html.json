{
  "hash": "8ecbc4f638361bfcc89881fd166daed1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Meta analysis for absolute beginners\"\ndate: 2024-06-25\ndescription: \"A hands-on introduction to the tip of the pyramid of evidence\"\n#image: \"downloads/PNG/grp-summarize-02@4x.png\"\ncategories:\n  - meta-analysis\n  - stats\n  - systematic review\n# citation: \n#   url: https://samanthacsik.github.io/posts/2024-03-31-data-viz-course/\ncitation: true\nbibliography: references.bib\ndraft: false # setting this to `true` will prevent your post from appearing on your listing page until you're ready!\n---\n\n\n\n\n\nThis is a beginners' introduction to meta-analysis, simply because I myself am quite a beginner. So this is all I can do I'm afraid. My colleague Sacha Altay and I tried ourselves on our first ever meta-analysis very recently. Our main research question was whether people can tell true news from false news. Quite a hot topic these days, it seems.\n\nThroughout this post, I'll use our study as an example to illustrate crucial steps of a meta-analysis. These include (i) the systematic literature search, (ii) standardized effect sizes, and (iii) meta-analytic models.\n\nThe idea is for this to be a hands-on introduction, so try to follow the instructions and perform parts of these steps yourself. I hope this will help you understood how a (very basic kind of) meta-analysis works, and give you some idea on how to perform one yourself.\n\n## Why care about meta-analysis ?\n\nYou have most likely heard about the \"Pyramid of evidence\". There are different versions of the pyramid, but they have one thing in common: at the very top, there are meta-analyses.\n\n![A typical textbook Pyramid of Evidence](images/pyramid.png){width=\"70%\"}\n\nThe basic idea of meta-analysis is easy: the more data the better. Results of single studies can be affected by a ton of factors, e.g. the country they were run in, the experimental setup the researchers, the sample they recruit and so on. But if we average across many studies on the same thing, we get a more robust, generalizable result. And comparing many studies, we might even get an idea of specific factors that are systematically associated with differences.\n\n## I The research question\n\nImagine our research question is: **\"Can people tell true news from false News\"**. And we know that there is psychological literature on that topic. These studies typically look like this:\n\n::::: columns\n::: {.column width=\"50%\"}\n![Example of items used by @pennycookPracticalGuideDoing2021](images/example-item.jpg){width=\"100%\"}\n:::\n\n::: {.column width=\"50%\"}\n![](images/example-item2.jpg){width=\"100%\"}\n:::\n:::::\n\nParticipants see a bunch of news headlines, sometimes with a lede and source, as they would appear on social media typically. Some of these headlines are false (typically identified as such by fact-checking sites such as Snopes), and some of them are true. For each news headline, they are asked to which extent they believe the headline to be accurate, (e.g. \"To the best of your knowledge, how accurate is the claim in the above headline\" 1 = Not at all accurate, 4 = Very accurate).\n\nIn order to do a meta-analysis, we need to have some quantifying measure for our research question. In this case, it is pretty straightforward. Researchers refer to people's capacity to tell true news from false news as news discernment, which is simply:\n\n$$\n\\text{discernment} = \\text{mean accuracy}_{\\text{true news}} - \\text{mean accuracy}_{\\text{false news}}\n$$\n\n<!-- $$ -->\n\n<!-- \\text{discernment} = \\frac{1}{n_t} \\sum_{i=1}^{n_t} \\text{accuracy}_{\\text{true news}, i} - \\frac{1}{n_f} \\sum_{i=1}^{n_f} \\text{accuracy}_{\\text{false news}, i} -->\n\n<!-- $$ -->\n\nWe have a research question, and we also already have an idea of a quantifiable answer. At this point, we would ideally write up a pre-registration. This is basically a plan for our meta-analysis project, including the literature search and analysis plan, but a plan that we commit to making public.\n\nHowever, it is hard to write up a pre-registration for a meta-analysis if you don't know anything about a meta-analysis. So we'll skip this step for now and jump right into effect sizes.\n\n## II Effect sizes\n\nIt might feel weird to talk about standardized effect sizes before the systematic literature search. But only if we know how to calculate (standardized) effect sizes, we can know what exactly we are looking for in articles.\n\nWe want to be able to compare discernment across studies. The problem is: different studies use different measurement scales. For example, @diasEmphasizingPublishersDoes2020 use a 4-point scale, while @roozenbeekSusceptibilityMisinformationCOVID192020 use a 7-point scale. How can we compare the results of the two?\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|ref             | mean_accuracy_true| mean_accuracy_fake| accuracy_scale_numeric| discernment|\n|:---------------|------------------:|------------------:|----------------------:|-----------:|\n|Dias_2020       |               2.57|               1.67|                      4|        0.90|\n|Roozenbeek_2020 |               5.40|               2.49|                      7|        2.91|\n\n\n:::\n:::\n\n\n\nThe solution are standardized effect sizes. Instead of expressing discernment on the original scale of the study, we express discernment in terms of standard deviations. All we have to do is divide the discernment score by the (pooled) standard deviation of true and false news. So on top of the mean ratings, we also need the standard deviations.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> ref </th>\n   <th style=\"text-align:right;\"> mean_accuracy_true </th>\n   <th style=\"text-align:right;\"> sd_accuracy_true </th>\n   <th style=\"text-align:right;\"> mean_accuracy_fake </th>\n   <th style=\"text-align:right;\"> sd_accuracy_fake </th>\n   <th style=\"text-align:right;\"> accuracy_scale_numeric </th>\n   <th style=\"text-align:right;\"> discernment </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Dias_2020 </td>\n   <td style=\"text-align:right;\"> 2.57 </td>\n   <td style=\"text-align:right;background-color: lightblue !important;\"> 0.52 </td>\n   <td style=\"text-align:right;\"> 1.67 </td>\n   <td style=\"text-align:right;background-color: lightblue !important;\"> 0.42 </td>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 0.90 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Roozenbeek_2020 </td>\n   <td style=\"text-align:right;\"> 5.40 </td>\n   <td style=\"text-align:right;background-color: lightblue !important;\"> 1.62 </td>\n   <td style=\"text-align:right;\"> 2.49 </td>\n   <td style=\"text-align:right;background-color: lightblue !important;\"> 1.34 </td>\n   <td style=\"text-align:right;\"> 7 </td>\n   <td style=\"text-align:right;\"> 2.91 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\nThe standardized discernment measure that we are after is generally referred to as a standardized mean difference (SMD), with perhaps the most popular version being called Cohenâ€™s d, named after the psychologist and statistician [Jacob Cohen](https://en.wikipedia.org/wiki/Jacob_Cohen_(statistician)).\n\nCohen's d is calculate as\n\n$$\n\\text{Cohen's d} = \\frac{\\bar{x}_{\\text{true}} - \\bar{x}_{\\text{false}}}{SD_{\\text{pooled}}}\n$$ with\n\n$$\nSD_{\\text{pooled}} = \\sqrt{\\frac{SD_{\\text{true}}^2+SD_{\\text{false}}^2}{2}}\n$$\n\nUsing these formula, we can now calculate our standardized discernment measure.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npooled_sd <- function(sd_true, sd_false) {\n  sd_pooled <- sqrt((sd_true^2 + sd_false^2) / 2)\n  return(sd_pooled)\n}\n\nmeta <- meta %>% \n  mutate(discernment = mean_accuracy_true - mean_accuracy_fake,\n         pooled_sd = pooled_sd(sd_accuracy_true, sd_accuracy_fake), \n         discernment_std = discernment/pooled_sd)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> ref </th>\n   <th style=\"text-align:right;\"> mean_accuracy_true </th>\n   <th style=\"text-align:right;\"> mean_accuracy_fake </th>\n   <th style=\"text-align:right;\"> sd_accuracy_fake </th>\n   <th style=\"text-align:right;\"> accuracy_scale_numeric </th>\n   <th style=\"text-align:right;\"> sd_accuracy_true </th>\n   <th style=\"text-align:right;\"> discernment </th>\n   <th style=\"text-align:right;\"> pooled_sd </th>\n   <th style=\"text-align:right;\"> discernment_std </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Dias_2020 </td>\n   <td style=\"text-align:right;\"> 2.57 </td>\n   <td style=\"text-align:right;\"> 1.67 </td>\n   <td style=\"text-align:right;\"> 0.42 </td>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 0.52 </td>\n   <td style=\"text-align:right;\"> 0.90 </td>\n   <td style=\"text-align:right;\"> 0.4726521 </td>\n   <td style=\"text-align:right;background-color: lightblue !important;\"> 1.904149 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Roozenbeek_2020 </td>\n   <td style=\"text-align:right;\"> 5.40 </td>\n   <td style=\"text-align:right;\"> 2.49 </td>\n   <td style=\"text-align:right;\"> 1.34 </td>\n   <td style=\"text-align:right;\"> 7 </td>\n   <td style=\"text-align:right;\"> 1.62 </td>\n   <td style=\"text-align:right;\"> 2.91 </td>\n   <td style=\"text-align:right;\"> 1.4866069 </td>\n   <td style=\"text-align:right;background-color: lightblue !important;\"> 1.957478 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\nGreat! We are almost ready to go to the next step and talk about meta-analytic models. But before that, there's one thing missing still: the standard error (SE).\n\n::: callout-note\nStandard error (SE) and standard deviation (SD) are not the same thing. SD measures the variability observed within a sample. In our case, e.g. distribution accuracy ratings of fake news. It can be calculated directly from the data. SE describes the variability of a (hypothetical) distribution of an effect. In our case, e.g. a distribution of mean accuracy ratings of fake news, each based on a different sample. We can only guess this distribution based on the data by relying on statistical theory.\n:::\n\nThe SE is like a measure of certainty. How certain can we be that we observe a certain discernment effect in a study not only because of sampling variation (e.g. the specific sample of participants we asked), but because there is actually a true effect in the population (in all people)? The bigger the sample, the smaller the SE, i.e. the more certain we can be that whatever we observe is actually representative of the population we want to make conclusions on.\n\n$$\nSE_{\\text{Cohen's d}} = SD_{\\text{pooled}}\\sqrt{\\frac{1}{n_\\text{true}}+\\frac{1}{n_\\text{false}}}\n$$\n\nwith $n_\\text{false}$ being the sample size of fake news items, $n_\\text{true}$ the sample size of true news items in group 1, and $SD_{\\text{pooled}}$ the **pooled standard deviation** of both groups (see above).\n\nIn our case, things are a bit tricky. Usually, in studies of news judgements, participants rate both false and true news items, and several of each category. First, this means that technically, we have to calculate the SE slightly differently. In the SE formula above, we were assuming independence of true and false news ratings. But the distributions of false and true news are necessarily correlated, because they are coming from the same participants. For this tutorial, we will ignore this. Second, our `n` are all instances of news ratings, i.e. is the product of the number of participants in our study (`n_subj`) and the number of news ratings per participant (`n_news`).\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|ref             | n_subj| n_news|    n|\n|:---------------|------:|------:|----:|\n|Dias_2020       |    187|     24| 4488|\n|Roozenbeek_2020 |    700|      8| 5600|\n\n\n:::\n:::\n\n\n\nWith this information, we can now calculate the SE. In our case, `n` is the combined number of rating instances. Let's just assume that participant saw equally many false news as true news, so that we have $n_\\text{true} = n_\\text{false} = n/2$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSE_Cohens_d <- function(sd_pooled, n_true, n_false) {\n  se_d <- sd_pooled * sqrt((1 / n_true) + (1 / n_false))\n  return(se_d)\n}\n\nmeta <- meta %>% \n  mutate(discernment_SE = SE_Cohens_d(sd_pooled = pooled_sd, n_true = n/2, n_false = n/2))\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> ref </th>\n   <th style=\"text-align:right;\"> pooled_sd </th>\n   <th style=\"text-align:right;\"> n </th>\n   <th style=\"text-align:right;\"> discernment_std </th>\n   <th style=\"text-align:right;\"> discernment_SE </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Dias_2020 </td>\n   <td style=\"text-align:right;\"> 0.4726521 </td>\n   <td style=\"text-align:right;\"> 4488 </td>\n   <td style=\"text-align:right;\"> 1.904149 </td>\n   <td style=\"text-align:right;background-color: lightblue !important;\"> 0.0141106 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Roozenbeek_2020 </td>\n   <td style=\"text-align:right;\"> 1.4866069 </td>\n   <td style=\"text-align:right;\"> 5600 </td>\n   <td style=\"text-align:right;\"> 1.957478 </td>\n   <td style=\"text-align:right;background-color: lightblue !important;\"> 0.0397312 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\nYay! Now that we have an effect size and its SE, we could run a meta-analysis.\n\nIf this was a bit tedious, here's some good news: We do not have to do this by hand every time. There are several R packages specifically dedicated to meta-analysis that you can use. For example, the [`metafor`](https://www.metafor-project.org/doku.php/metafor) package has a function called [`escalc()`](https://wviechtb.github.io/metafor/reference/escalc.html#-outcome-measures-for-two-group-comparisons) to calculate various effect sizes that are commonly used in meta-analyses. Here is an example of how to do what we just did using escalc:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(metafor)\n\n# Calculate SMD and its variance\nmeta_effect_sizes <- escalc(measure = \"SMD\", \n                  # diff = true (m1i) - fake (m2i)\n                  m1i=mean_accuracy_true,\n                  sd1i=sd_accuracy_true, \n                  m2i= mean_accuracy_fake, \n                  sd2i=sd_accuracy_fake, \n                  n1i = n_observations/2,\n                  n2i = n_observations/2, \n                  data = meta)\n\n# View the resulting data frame with effect sizes and variances\nmeta_effect_sizes %>% \n  select(ref, starts_with(\"mean\"), yi, vi) %>% \n  mutate(SE = sqrt(vi)) %>% \n  kable()\n```\n\n::: {.cell-output-display}\n\n\n|ref             | mean_accuracy_fake| mean_accuracy_true|       yi|        vi|        SE|\n|:---------------|------------------:|------------------:|--------:|---------:|---------:|\n|Dias_2020       |               1.67|               2.57| 1.903831| 0.0012928| 0.0359551|\n|Roozenbeek_2020 |               2.49|               5.40| 1.957216| 0.0010563| 0.0325010|\n\n\n:::\n:::\n\n\n\nIn this output, `yi`is the SDM (i.e. our standardized discernment effect), and `vi` is the variance (which is just $SE^2$). The values for `SE` are not exactly the same we obtained when calculating Cohen's D by hand before, because `escalc()` uses not Cohen's D, but Hedges' G (a very similar measure but with some small sample correction).\n\nThere are many more standardized effect sizes. Which one you want to use depends mostly on the kind of data you want to analyze. For dichotomous outcomes, researchers typically use (log) odds or risk ratios, for associations between two continuous variables simply a correlation[^1] and so on. There is a great introduction to doing meta-analysis by @harrerDoingMetaAnalysisHandsOn2021. The [book is freely available online](https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/) and includes r-code. There is a [whole chapter on effect sizes](https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/effects.html#effects) which you might want to look at.\n\n[^1]: often researchers use a z-transformed version of a correlation coefficient\n\nWe know have an idea of what we need to extract from the papers, in order to be able to run a meta-analysis:\n\n-   the means and standard deviations of false and true news ratings, respectively.\n-   the number of participants and the number of news ratings per participant\n\nWith that in mind, we are ready to dive into the literature search.\n\n## III Systematic literature search\n\nIdeally, we want all studies that have ever been written on our research question. Remember, the more the better. There's just one issue: try to type something like \"misinformation\" or \"fake news\" in google scholar.\n\n![Screenshot from a google scholar search on June 27, 2024](images/scholar.png){width=\"70%\"}\n\nUps. Not enough time to review all these in a life time probably. The first thing we should do is be a bit more specific in what we're looking for. So let's refine our search string.\n\n### Search string\n\nYou will most likely never start a systematic literature review having absolutely no clue about the topic you care about. There might at least be that one paper that inspired your idea for a meta-analysis. This paper gives you some first ideas of keywords that you could look for.\n\nIn our case, we settled on this search string\n\n> '\"false news\" OR \"fake news\" OR \"false stor\\*\" AND \"accuracy\" OR \"discernment\" OR \"credibilit\\*\" OR \"belief\" OR \"susceptib\\*\"'\n\nThere are a couple of things here. We used 'OR's and 'AND's to combine words. These are Boolean operators. Quick quizz:\n\n:::: quiz\n<i class=\"fa fa-question-circle quiz-icon\"></i>\n\n<div>\n\nImagine we would have run a title-search (i.e. not searching abstracts or other content, but only the headlines of articles). Would this search string have yielded a study called \"News accuracy ratings of US adults during 2016 presidential elections\"?\n\n</div>\n::::\n\n::::: columns\n::: {.column width=\"50%\"}\n![Boolean AND operator. Only if both keywords are included, a result will show up.](images/boolean_operator_AND.png){width=\"100%\"}\n:::\n\n::: {.column width=\"50%\"}\n![Boolean OR operator. As long as one of the keywords is included, a result will show up.](images/boolean_operator_OR.png){width=\"100%\"}\n:::\n:::::\n\nNo. And that is because of the boolean operators. The \"OR\" operator tells the search engine that as long as one of the keywords is matched, return the result. By contrast, the AND operator tells the search engine, that both keywords need to be matched. For example, \"accuracy\" **OR** \"discernment\" would return an article entitled \"News accuracy ratings of US adults during 2016 presidential elections\", whereas \"accuracy\" **AND** \"discernment\" would not.\n\nInstead of single keywords, you can also link combinations of keywords. Our search string, put a bit more abstractly, reads ...OR...OR...AND...Or...OR... As in math, there is a hierarchy among operators. On Scopus (the search engine we used), [OR operators are treated before AND operators](https://schema.elsevier.com/dtds/document/bkapi/search/SCOPUSSearchTips.htm). It's like in math, when you know that $1 + 2*4 = 9$, because you multiply before you add. You could re-write the math term $1 + (2*4) = 9$ to make the convention explicit, and you could do the same for the search string (...OR...OR...) AND (...Or...OR...), but you don't have to.\n\nImagine that we are happy with the combination of keywords that we have in our search string. Where do we look for articles now?\n\n### Data bases\n\nYou will all know Google Scholar, which is very convenient for quick searches. We all use it regularly to find a specific article that we are looking for, or to get a first impression of results from some keywords.\n\nThe one big disadvantage for google scholar is that the [results (and least in terms of order) are user-specific](https://libguides.kcl.ac.uk/systematicreview/searchdb). You and I running the same search query on google scholar, results will not be listed in the same way. This is problematic, because literature of a systematic review should ideally be reproducible.\n\nLuckily, there are loads of other data bases that you can search. Some of them are issue specific (e.g. Pubmed for medicine in the broadest sense) while others are more general (e.g. Scopus, Web of Science).\n\n::: callout-note\nMost of these databases will miss non-published data. Researchers still have a hard time of publishing null findings, and very recent pre-prints have not yet made it through peer-review. Only searching peer-reviewed, published records might therefor bias your selection of studies. To counteract, you can additionally search on pre-print servers, e.g. [PsyArXiv](https://osf.io/preprints/psyarxiv) for psychology.\n:::\n\nIn our case, say we want to search Scopus, a very large and general data base.\n\n![](images/scopus_1.png){width=\"100%\"}\n\nWe can enter our search term, and specify what parts of the articles we want to search for. We will pick Article title, Abstract & Keywords.\n\nOnce we click on search, we see the number of overall search results, and a list of articles.\n\n![](images/scopus_2.png){width=\"100%\"}\n\nThe number of results is less shocking than the initial one from searching \"misinformation\" on Google Scholar. But it is still a whole lot of articles, and we couldn't possibly read all of them entirely.\n\nWe can use some more refined criteria to filter out some likely irrelevant results[^2]. In the left hand panel, we can select for example the document type, language, or subject areas. We can exclude, for example, a couple of disciplines that are most likely not relevant, such as \"Chemistry\".\n\n[^2]: these are part of in-(or ex-)clusion criteria, but let's leave that for later\n\n::::: columns\n::: {.column width=\"50%\"}\n![](images/scopus_3.png){width=\"50%\"}\n:::\n\n::: {.column width=\"50%\"}\n![](images/scopus_4.png){width=\"60%\"}\n:::\n:::::\n\nThe more restrictions we make, the more it will reduce the number of search results. You will also not that adding these restrictions changes the search string (you can see the full string by clicking on the orange \"Advanced query\" button).\n\n![](images/scopus_5.png){width=\"100%\"}\n\nIf we're finally happy with our search, we need to save the search results somewhere[^3]. Some people like to export all results to a reference manager such as Zotero. For our project, I preferred not to flood my Zotero and use the convenient options to export search results to a `.csv` file.\n\n[^3]: On scopus, you can additionally save your search history and results by making an account.\n\n::::: columns\n::: {.column width=\"50%\"}\n![Boolean AND operator. Only if both keywords are included, a result will show up.](images/scopus_6.png){width=\"50%\"}\n:::\n\n::: {.column width=\"50%\"}\n![Boolean OR operator. As long as one of the keywords is included, a result will show up.](images/scopus_7.png){width=\"60%\"}\n:::\n:::::\n\n:::: exercise\n<i class=\"fa fa-computer exercise-icon\"></i>\n\n<div>\n\nTime to run your own literature search:\n\n1.  Find a partner\n2.  Make up a search string\n3.  Run it on Scopus\n4.  Restrict search results\n5.  Download a `.csv` file with the first 100 results\n\n</div>\n::::\n\nNow that we have all results stored, there are still many, many articles. Do we have to read them all? No. Before we start reading entire papers, we do screening.\n\n### Screening\n\nThere are usually two stages of screening: (i) title screening and (ii) abstract screening. Since screening decisions can sometimes be quite arbitrary, we ideally want several researchers to do it independently.\n\n#### Title screening\n\nDuring title screening we throw out titles that are very obviously not relevant.\n\n:::: exercise\n<i class=\"fa fa-computer exercise-icon\"></i>\n\n<div>\n\nDo your:\n\n1.  Stay with your partner\n2.  Upload your `.csv` search result file in a google spreadsheet\n3.  Limit your spreadsheet to the first 30 results.\n4.  Make a new tab and copy only the title column. Make a second column called \"reviewer\". Make a third column \"decision\". Duplicate this tab.\n5.  Each reviewer screens all titles independently. Put your respective initals in all cells of the \"reviewer column\". Agree on a code for in/exclusion in the \"decision\" column.\n6.  Compare with your partner and explain cases of exclusion\n\n</div>\n::::\n\nThere will almost certainly be some cases where you and your co-reviewer will not agree.[^4].\n\n[^4]: There are ways to quantify the agreement between raters (e.g. [Cohen's kappa](https://en.wikipedia.org/wiki/Cohen%27s_kappa)), but we don't bother much about that here.\n\nOne, conservative solution to proceed is to only remove titles that all reviewers found to be irrelevant, and make all others pass through to the next screening stage.\n\n#### Abstract screening\n\nThe next stage, abstract screening, is a bit more challenging. At this point, the any search result that we retained might be relevant to review for us. That means, from now on, we really need to justify why we include some articles, but not others.\n\nHere are some criteria that we can set:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n| CriterionID|Criterion_type    |Definition                                                                                                                                                     |\n|-----------:|:-----------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------|\n|         1.0|Document type     |All literature with original data: peer reviewed papers, but also pre-prints (this implies unique data that is not otherwise already included in the analysis) |\n|         2.0|Research question |is about the perceived accuracy of fake AND true news                                                                                                          |\n|         3.0|Access            |is the paper available                                                                                                                                         |\n|         4.0|Method            |measures accuracy or an equivalent construct (e.g. trustworthiness, truthfulness)                                                                              |\n|         4.1|Method            |provides all relevant summary statistics (i.e. accuracy ratings for both fake AND true news) or data to produce them; a mere discernment score is not enough   |\n|         4.2|Method            |provides a clear control condition that is comparable with those of other studies                                                                              |\n|         5.0|Stimuli           |real world news only                                                                                                                                           |\n\n\n:::\n:::\n\n\n\nThese inclusion[^5] criteria are very important. For every article that we reject after the title screening, we use them to justify why we excluded them.\n\n[^5]: or exclusion, depending on how you want to frame it\n\n::: callout-note\nThe restrictions we made on our data base are technically also inclusion criteria. They are irrelevant for the screening, because they were already used to filter the results. But you need to report them.\n:::\n\nInclusion criteria might be evolving during your screening process. Most likely, you weren't completely clear on everything you want or not, before launching the literature search. That's fine, as long as you are transparent about it (see section of pre-registration).\n\n:::: exercise\n<i class=\"fa fa-computer exercise-icon\"></i>\n\n<div>\n\n[Here is a google spreadsheet with 10 of abstracts to screen](https://docs.google.com/spreadsheets/d/1slskn-AwoscEufYPslzdtWF-JHvtVuyIigIWPhpfsgc/edit?gid=230624631#gid=230624631). With you partner:\n\n1.  Download the `.csv` or make a copy in Google Spreadsheets\n2.  Read the abstracts.\n3.  For each abstract, give your inclusion decision. If you reject, justify.\n4.  Compare your decisions\n\n</div>\n::::\n\n### Full text assessment\n\nNow, finally, it is time to read full articles.\n\n:::: exercise\n<i class=\"fa fa-computer exercise-icon\"></i>\n\n<div>\n\nWithin your groups:\n\n1.  Pick one of these two articles each, i.e. one per person.\n\n-   Dias, N., Pennycook, G., & Rand, D. G. (2020). Emphasizing publishers does not effectively reduce susceptibility to misinformation on social media. Harvard Kennedy School Misinformation Review. https://doi.org/10.37016/mr-2020-001\n-   Roozenbeek, J., Schneider, C. R., Dryhurst, S., Kerr, J., Freeman, A. L. J., Recchia, G., van der Bles, A. M., & van der Linden, S. (2020). Susceptibility to misinformation about COVID-19 around the world. Royal Society Open Science, 7(10), 201199. https://doi.org/10.1098/rsos.201199\n\n2.  Access the articles via the usual means you find and download papers.\n3.  Think about whether you include them.\n4.  Extract the information you need.\n\n</div>\n::::\n\n### Pre-registration\n\nIdeally, you should already preregister after you are clear on your research question and effect sizes. There are [some good templates](https://docs.google.com/document/d/1fU-EY36gId9FWHsP4-lE2SF9IVCFBMKUzBJmHD9bBYQ/edit#heading=h.fwbi14d4b65g), e.g. on the Open Science Framework (OSF).\n\n### PRISMA\n\nWhat we just did follows pretty closely the [PRISMA guidelines](https://www.prisma-statement.org/). Once you are done with your systematic review, you should use their overview template to communicate your literature review and its different stages.\n\n## IV Meta-analysis\n\nHere's an intuitive way to think about a meta-analysis: We just take the effect sizes of all the studies and then calculate an some average. That average, we hope, is closer to the \"true effect\" than any of the single studies.\n\nThe problem is: Some studies have a way larger sample than others. Our average should take this into account--in other words, what we want is a *weighted average*. The bigger the sample--and thus the smaller the standard error--the more weight a study should have.\n\n\n\n\n\n\n\n### Fixed-effect vs. Random-effect model\n\nThe way we calculate these weights is where things become a bit philosophical. There are two models to conceive of our study effect sizes: (i) a fixed effect model and (ii) a random effects model. The fixed-effect model assumes that there is one \"true\" effect only. Effect sizes vary from one study to another, but only becasue sampling variation. The random-effects model assumes that there are several \"true\" effects. Studies vary because of sampling variation, but also because they have different \"true\" effects.\n\nIf this sounds confusing, you're not alone. I like to imagine it like this: Imagine you run the same study on news-discernment both in the US and in China. You get very different results. According to the fixed-effect model, these are two samples from the same population--all people--and there is one true value for news discernment capacity across the globe. According to the random-effects model, there these are two different populations, who have two distinct true values for news discernment capacity.\n\nIf this still isn't clear (I wouldn't blame you), you can [give this chapter a read](https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/pooling-es.html#pooling-es). The take away is that for many meta-analyses (at least in social sciences), a random-effects model is probably the better choice.\n\n> \"\\[...\\] there may be countless reasons why real differences exist in the true effect sizes of studies. The random-effects model addresses this concern. It provides us with a model that often reflects the reality behind our data much better.\" [@harrerDoingMetaAnalysisHandsOn2021]\n\n### Weighted averages\n\nRemember that our meta-analysis ultimately leaves us with some sort of weighted average. We want studies with larger samples (and thus smaller SEs, or sampling variation) to have more weight.\n\nThe most common approach to do this is often called inverse-variance weighting or simply inverse-variance meta-analysis [@harrerDoingMetaAnalysisHandsOn2021].\n\n$$\nw_k = \\frac{1}{s^2_k}\n$$ where $w_k$ is the weight for each study $k$ and $s^2_k$ is the variance (the squared standard error) of each effect size.\n\nBased on these weights, the meta-analytic average is calculated as the sum of weighted effect sizes divided by the sum o fall individual weights\n\n$$\n\\hat\\theta = \\frac{\\sum^{K}_{k=1} \\hat\\theta_kw_k}{\\sum^{K}_{k=1} w_k}\n$$\n\nwith $\\hat\\theta$ estimate of the true pooled effect and $\\hat\\theta_k$ each study's effect size.\n\nFor the random effects model, it's a little more difficult. It assumes that on top of the sampling variation, there is also other \"random\" variance $\\tau^2$, or **tau-squared**.\n\n$$\nw^*_k = \\frac{1}{s^2_k+\\tau^2} \n$$ Estimating $\\tau^2$ [is possible but but complex](https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/pooling-es.html#tau-estimators). Luckily, there are packages that do this for us. After determining the correct weight, random-effects models typically also use the same approach of inverse variance method as described above.\n\nLet's try an example of how to do a random-effects meta-analysis with the `metafor` package. Before we can run a meta-analysis, we need to make sure to have a data-frame with effect sizes.\n\n\n\n\n\n\n\n:::: exercise\n<i class=\"fa fa-computer exercise-icon\"></i>\n\n<div>\n\n[Here is a spreadsheet](https://docs.google.com/spreadsheets/d/1slskn-AwoscEufYPslzdtWF-JHvtVuyIigIWPhpfsgc/edit?gid=697316395#gid=697316395).\n\n1.  Download it.\n\n2.  Calculate effect sizes using the `escalc()` function from the metafor package.\n\n</div>\n::::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(metafor)\n\nmeta <- read_csv(\"data/meta_randomeffects_example.csv\")\n\n# Calculate SMD and its variance\nmeta_effect_sizes <- escalc(measure = \"SMD\", \n                  # diff = true (m1i) - fake (m2i)\n                  m1i=mean_accuracy_true,\n                  sd1i=sd_accuracy_true, \n                  m2i= mean_accuracy_fake, \n                  sd2i=sd_accuracy_fake, \n                  n1i = n/2,\n                  n2i = n/2, \n                  data = meta) %>% \n  arrange(yi)\n\n# View the resulting data frame \nmeta_effect_sizes %>% \n  kable()\n```\n\n::: {.cell-output-display}\n\n\n| paperID|ref              | mean_accuracy_fake| mean_accuracy_true| sd_accuracy_fake| sd_accuracy_true|     n|        yi|        vi|\n|-------:|:----------------|------------------:|------------------:|----------------:|----------------:|-----:|---------:|---------:|\n|       7|Guess_2020       |               2.40|               2.77|            0.729|            0.694| 19638| 0.5198510| 0.0002106|\n|       1|Altay_2022       |               1.99|               2.71|            0.890|            0.880|  2990| 0.8133421| 0.0014484|\n|       2|Lutzke_2019      |               4.28|               6.38|            2.550|            2.340|  5604| 0.8579899| 0.0007795|\n|      10|Luo_2022         |               2.98|               4.72|            1.950|            2.010|  1150| 0.8781128| 0.0038135|\n|       4|Pennycook_2020   |               1.95|               2.64|            0.600|            0.530|  4020| 1.2186749| 0.0011797|\n|       8|Pennycook_2020_b |               0.33|               0.65|            0.280|            0.220| 12795| 1.2708078| 0.0003757|\n|       9|Clayton_2020     |               1.96|               3.34|            0.934|            0.927|  4221| 1.4827995| 0.0012081|\n|       3|Pennycook_2018   |               1.78|               2.83|            0.600|            0.700|  2847| 1.6102018| 0.0018603|\n|       6|Pennycook_2019   |               1.66|               2.59|            0.450|            0.440| 63456| 2.0897310| 0.0000974|\n|       5|Bronstein_2019   |               1.79|               2.78|            0.460|            0.470| 12048| 2.1287766| 0.0005201|\n\n\n:::\n:::\n\n\n\nWith the effect sizes, we can now run our meta-analysis.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresult <- metafor::rma(yi, vi, data = meta_effect_sizes, slab = ref)\nmetafor::forest(result)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/random-effects-metaanalysis-1.png){width=672}\n:::\n:::\n\n\n\nYay. This plot is called a \"forest plot\". We can see all effect sizes of the the studies that we fed into the model. In the right en column we see all standardizes discernment effect sizes and their confidence intervals. In the bottom row, we see the row label \"RE Model\", for random-effects model, the value for our meta-analytic average.\n\n### Multi-level\n\nSo far so good. Things get more complicated if we have more than one effect size per study, and perhaps even per sample within a study. For our studies, this is the case.\n\n\n\n\n\n\n\nOne of the reasons is that participants did not only see several news items, but sometimes also news items of different categories, such as politically concordant (e.g. pro-republican news for a Republican participant) and discordant (e.g. pro-democrat news for a Republican participant).\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n| paperID|ref            | sampleID|news_family            |        n|         yi|        vi|\n|-------:|:--------------|--------:|:----------------------|--------:|----------:|---------:|\n|      41|Eun-Ju_2023    |        3|politically_discordant |   320.00| -0.2660372| 0.0126106|\n|      41|Eun-Ju_2023    |       12|politically_concordant |   400.00| -0.2505704| 0.0100785|\n|      41|Eun-Ju_2023    |        6|politically_concordant |   320.00| -0.1423425| 0.0125317|\n|      41|Eun-Ju_2023    |        9|politically_discordant |   400.00|  0.0000000| 0.0100000|\n|      41|Eun-Ju_2023    |        9|politically_concordant |   400.00|  0.0854359| 0.0100091|\n|      69|Shirikov_2024  |        3|politically_concordant | 72880.48|  0.1293124| 0.0000550|\n|      56|Garrett_2021   |        1|politically_concordant |  2212.00|  0.1371033| 0.0018126|\n|      41|Eun-Ju_2023    |        6|politically_discordant |   320.00|  0.1396615| 0.0125305|\n|      69|Shirikov_2024  |        3|politically_discordant | 72880.48|  0.1771236| 0.0000551|\n|      41|Eun-Ju_2023    |       12|politically_discordant |   400.00|  0.1784714| 0.0100398|\n|      69|Shirikov_2024  |        4|politically_discordant | 14625.28|  0.2161790| 0.0002751|\n|      69|Shirikov_2024  |        4|politically_concordant | 14625.28|  0.2523929| 0.0002757|\n|      58|Lyons_2024     |        1|politically_discordant |  4848.00|  0.2843118| 0.0008334|\n|      58|Lyons_2024     |        1|politically_concordant |  4842.00|  0.3126600| 0.0008362|\n|      58|Lyons_2024     |        3|politically_discordant |  4701.00|  0.3145489| 0.0008614|\n|      58|Lyons_2024     |        3|politically_concordant |  4692.00|  0.3325030| 0.0008643|\n|      64|Guess_2024     |        5|politically_concordant |   810.00|  0.3376436| 0.0050086|\n|      55|Gawronski_2023 |        2|politically_discordant |  8850.00|  0.3398163| 0.0004585|\n|      58|Lyons_2024     |        1|politically_discordant |  4788.00|  0.3517364| 0.0008483|\n|      64|Guess_2024     |        5|politically_discordant |   810.00|  0.3631048| 0.0050197|\n\n\n:::\n:::\n\n\n\nAs a result, for each sample, we have two observations, i.e two lines in our data frame. Because observations are not independent of each other, and a standard meta-analysis assumes independence, we have to find a way of modelling this dependency. This is where multi-level meta-analysis becomes relevant.\n\nIn fact, the standard random-effects model that we've learned about already is a multi-level model. It has two levels of variation: the sampling variance (the SE), and some unique study variance (what we labeled 'tau'). Now we want to add an additional level between these two, namely some unique sample variance.\n\nThere is [a great chapter on multi-level models](https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/multilevel-ma.html) in @harrerDoingMetaAnalysisHandsOn2021. The key take-away is that as soon as there are dependencies in our data frame, we need to model them. If effect sizes are correlated and we do not model the sources of that correlation, this can artificially reduce the heterogeniety of our effect sizes and lead to false-positive results [@harrerDoingMetaAnalysisHandsOn2021]. For example, if we run ten studies but 8 on them rely on the same 100 participants, we might find results that are very similar across studies, but that's precisely because they can hardly be seen as independent studies.\n\nImplementing a multilevel meta-analysis with the `metafor` package in R is pretty intuitive, if you have the habit of running linear mixed-models. The syntax is very similar.\n\nFirst, we crate add an `observation_id` to our data frame, which identifies each row, i.e. each effect size in our sample. We then create a `unique_sample_id`, which is a combination of the `ref` and the `sampleID` variable.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmeta <- meta %>% \n  mutate(observation_id = 1:nrow(.), \n         unique_sample_id = paste0(ref, sampleID))\n\nhead(meta) %>% \n  kable() %>%\n  column_spec(c(8, 9), background = \"lightblue\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> paperID </th>\n   <th style=\"text-align:left;\"> ref </th>\n   <th style=\"text-align:right;\"> sampleID </th>\n   <th style=\"text-align:left;\"> news_family </th>\n   <th style=\"text-align:right;\"> n </th>\n   <th style=\"text-align:right;\"> yi </th>\n   <th style=\"text-align:right;\"> vi </th>\n   <th style=\"text-align:right;\"> observation_id </th>\n   <th style=\"text-align:left;\"> unique_sample_id </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 41 </td>\n   <td style=\"text-align:left;\"> Eun-Ju_2023 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:left;\"> politically_discordant </td>\n   <td style=\"text-align:right;\"> 320.00 </td>\n   <td style=\"text-align:right;\"> -0.2660372 </td>\n   <td style=\"text-align:right;\"> 0.0126106 </td>\n   <td style=\"text-align:right;background-color: lightblue !important;\"> 1 </td>\n   <td style=\"text-align:left;background-color: lightblue !important;\"> Eun-Ju_20233 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 41 </td>\n   <td style=\"text-align:left;\"> Eun-Ju_2023 </td>\n   <td style=\"text-align:right;\"> 12 </td>\n   <td style=\"text-align:left;\"> politically_concordant </td>\n   <td style=\"text-align:right;\"> 400.00 </td>\n   <td style=\"text-align:right;\"> -0.2505704 </td>\n   <td style=\"text-align:right;\"> 0.0100785 </td>\n   <td style=\"text-align:right;background-color: lightblue !important;\"> 2 </td>\n   <td style=\"text-align:left;background-color: lightblue !important;\"> Eun-Ju_202312 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 41 </td>\n   <td style=\"text-align:left;\"> Eun-Ju_2023 </td>\n   <td style=\"text-align:right;\"> 6 </td>\n   <td style=\"text-align:left;\"> politically_concordant </td>\n   <td style=\"text-align:right;\"> 320.00 </td>\n   <td style=\"text-align:right;\"> -0.1423425 </td>\n   <td style=\"text-align:right;\"> 0.0125317 </td>\n   <td style=\"text-align:right;background-color: lightblue !important;\"> 3 </td>\n   <td style=\"text-align:left;background-color: lightblue !important;\"> Eun-Ju_20236 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 41 </td>\n   <td style=\"text-align:left;\"> Eun-Ju_2023 </td>\n   <td style=\"text-align:right;\"> 9 </td>\n   <td style=\"text-align:left;\"> politically_discordant </td>\n   <td style=\"text-align:right;\"> 400.00 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n   <td style=\"text-align:right;\"> 0.0100000 </td>\n   <td style=\"text-align:right;background-color: lightblue !important;\"> 4 </td>\n   <td style=\"text-align:left;background-color: lightblue !important;\"> Eun-Ju_20239 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 41 </td>\n   <td style=\"text-align:left;\"> Eun-Ju_2023 </td>\n   <td style=\"text-align:right;\"> 9 </td>\n   <td style=\"text-align:left;\"> politically_concordant </td>\n   <td style=\"text-align:right;\"> 400.00 </td>\n   <td style=\"text-align:right;\"> 0.0854359 </td>\n   <td style=\"text-align:right;\"> 0.0100091 </td>\n   <td style=\"text-align:right;background-color: lightblue !important;\"> 5 </td>\n   <td style=\"text-align:left;background-color: lightblue !important;\"> Eun-Ju_20239 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 69 </td>\n   <td style=\"text-align:left;\"> Shirikov_2024 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:left;\"> politically_concordant </td>\n   <td style=\"text-align:right;\"> 72880.48 </td>\n   <td style=\"text-align:right;\"> 0.1293124 </td>\n   <td style=\"text-align:right;\"> 0.0000550 </td>\n   <td style=\"text-align:right;background-color: lightblue !important;\"> 6 </td>\n   <td style=\"text-align:left;background-color: lightblue !important;\"> Shirikov_20243 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\nWe can then proceed and run our multi-level model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Multilevel random effect model for accuracy\nmultilevel_model <-  metafor::rma.mv(yi, vi, random = ~ 1 | unique_sample_id / observation_id, data=meta)\n```\n:::\n\n\n\nOne additional thing, which I will not further discuss unfortunately, but which you should know about ([you can look here for more info](https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/multilevel-ma.html#rve)): Multi-level models do not account for dependencies in sampling error. But when one same sample contributes several effect sizes, we should expect their respective sampling errors to be correlated [@harrerDoingMetaAnalysisHandsOn2021], too. To account for dependency in sampling errors, you should compute cluster-robust standard errors.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrobust_multilevel_model <- metafor::robust(multilevel_model, cluster = unique_sample_id)\n```\n:::\n\n\n\n### Meta-regression\n\nIn meta-analyses, we often want to go beyond mere averages. We also like to know what factors contribute to the differences we observe between effect sizes. Meta-regressions allow us to do exactly that.\n\nThe good new is that this is pretty easy to implement and understand (at after having worked our brains to try to understand multi-level models). It works just like any normal regression analysis.\n\nLet's say we want to know differences in discernment associated with political concordance. All we have to do is add a term for that variable to our model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Multilevel random effect model for accuracy\nmeta_regression_model <-  metafor::rma.mv(yi, vi, \n                                          # you can also add multiple covariates\n                                          mods = ~news_family, \n                                          random = ~ 1 | unique_sample_id / observation_id, \n                                          data=meta)\n\nmodelsummary::modelsummary(meta_regression_model, \n                           stars = TRUE)\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<!-- preamble start -->\n\n    <script>\n      function styleCell_xyiwpkap39beiuyf1997(i, j, css_id) {\n        var table = document.getElementById(\"tinytable_xyiwpkap39beiuyf1997\");\n        table.rows[i].cells[j].classList.add(css_id);\n      }\n      function insertSpanRow(i, colspan, content) {\n        var table = document.getElementById('tinytable_xyiwpkap39beiuyf1997');\n        var newRow = table.insertRow(i);\n        var newCell = newRow.insertCell(0);\n        newCell.setAttribute(\"colspan\", colspan);\n        // newCell.innerText = content;\n        // this may be unsafe, but innerText does not interpret <br>\n        newCell.innerHTML = content;\n      }\n      function spanCell_xyiwpkap39beiuyf1997(i, j, rowspan, colspan) {\n        var table = document.getElementById(\"tinytable_xyiwpkap39beiuyf1997\");\n        const targetRow = table.rows[i];\n        const targetCell = targetRow.cells[j];\n        for (let r = 0; r < rowspan; r++) {\n          // Only start deleting cells to the right for the first row (r == 0)\n          if (r === 0) {\n            // Delete cells to the right of the target cell in the first row\n            for (let c = colspan - 1; c > 0; c--) {\n              if (table.rows[i + r].cells[j + c]) {\n                table.rows[i + r].deleteCell(j + c);\n              }\n            }\n          }\n          // For rows below the first, delete starting from the target column\n          if (r > 0) {\n            for (let c = colspan - 1; c >= 0; c--) {\n              if (table.rows[i + r] && table.rows[i + r].cells[j]) {\n                table.rows[i + r].deleteCell(j);\n              }\n            }\n          }\n        }\n        // Set rowspan and colspan of the target cell\n        targetCell.rowSpan = rowspan;\n        targetCell.colSpan = colspan;\n      }\nwindow.addEventListener('load', function () { styleCell_xyiwpkap39beiuyf1997(0, 0, 'tinytable_css_idxktibfjh57zui11gvoqr') })\nwindow.addEventListener('load', function () { styleCell_xyiwpkap39beiuyf1997(0, 1, 'tinytable_css_ideiwm9gfnzpk1hitq6w39') })\nwindow.addEventListener('load', function () { styleCell_xyiwpkap39beiuyf1997(1, 0, 'tinytable_css_idfkqsu1nm232ezthd7d83') })\nwindow.addEventListener('load', function () { styleCell_xyiwpkap39beiuyf1997(1, 1, 'tinytable_css_idyjs4ff4s939y3fgvycqn') })\nwindow.addEventListener('load', function () { styleCell_xyiwpkap39beiuyf1997(2, 0, 'tinytable_css_idfkqsu1nm232ezthd7d83') })\nwindow.addEventListener('load', function () { styleCell_xyiwpkap39beiuyf1997(2, 1, 'tinytable_css_idyjs4ff4s939y3fgvycqn') })\nwindow.addEventListener('load', function () { styleCell_xyiwpkap39beiuyf1997(3, 0, 'tinytable_css_idfkqsu1nm232ezthd7d83') })\nwindow.addEventListener('load', function () { styleCell_xyiwpkap39beiuyf1997(3, 1, 'tinytable_css_idyjs4ff4s939y3fgvycqn') })\nwindow.addEventListener('load', function () { styleCell_xyiwpkap39beiuyf1997(4, 0, 'tinytable_css_id6o1br5r04ruezdh0mf4p') })\nwindow.addEventListener('load', function () { styleCell_xyiwpkap39beiuyf1997(4, 1, 'tinytable_css_idhrt41gyjb59skacb9aoz') })\nwindow.addEventListener('load', function () { styleCell_xyiwpkap39beiuyf1997(5, 0, 'tinytable_css_idfkqsu1nm232ezthd7d83') })\nwindow.addEventListener('load', function () { styleCell_xyiwpkap39beiuyf1997(5, 1, 'tinytable_css_idyjs4ff4s939y3fgvycqn') })\nwindow.addEventListener('load', function () { styleCell_xyiwpkap39beiuyf1997(6, 0, 'tinytable_css_idfkqsu1nm232ezthd7d83') })\nwindow.addEventListener('load', function () { styleCell_xyiwpkap39beiuyf1997(6, 1, 'tinytable_css_idyjs4ff4s939y3fgvycqn') })\nwindow.addEventListener('load', function () { styleCell_xyiwpkap39beiuyf1997(7, 0, 'tinytable_css_idfkqsu1nm232ezthd7d83') })\nwindow.addEventListener('load', function () { styleCell_xyiwpkap39beiuyf1997(7, 1, 'tinytable_css_idyjs4ff4s939y3fgvycqn') })\nwindow.addEventListener('load', function () { styleCell_xyiwpkap39beiuyf1997(8, 0, 'tinytable_css_idfkqsu1nm232ezthd7d83') })\nwindow.addEventListener('load', function () { styleCell_xyiwpkap39beiuyf1997(8, 1, 'tinytable_css_idfkqsu1nm232ezthd7d83') })\n    </script>\n\n    <style>\n    .table td.tinytable_css_idxktibfjh57zui11gvoqr, .table th.tinytable_css_idxktibfjh57zui11gvoqr {  text-align: left;  border-bottom: solid 0.1em #d3d8dc; }\n    .table td.tinytable_css_ideiwm9gfnzpk1hitq6w39, .table th.tinytable_css_ideiwm9gfnzpk1hitq6w39 {  text-align: center;  border-bottom: solid 0.1em #d3d8dc; }\n    .table td.tinytable_css_idfkqsu1nm232ezthd7d83, .table th.tinytable_css_idfkqsu1nm232ezthd7d83 {  text-align: left; }\n    .table td.tinytable_css_idyjs4ff4s939y3fgvycqn, .table th.tinytable_css_idyjs4ff4s939y3fgvycqn {  text-align: center; }\n    .table td.tinytable_css_id6o1br5r04ruezdh0mf4p, .table th.tinytable_css_id6o1br5r04ruezdh0mf4p {  border-bottom: solid 0.05em black;  text-align: left; }\n    .table td.tinytable_css_idhrt41gyjb59skacb9aoz, .table th.tinytable_css_idhrt41gyjb59skacb9aoz {  border-bottom: solid 0.05em black;  text-align: center; }\n    </style>\n    <div class=\"container\">\n      <table class=\"table table-borderless\" id=\"tinytable_xyiwpkap39beiuyf1997\" style=\"width: auto; margin-left: auto; margin-right: auto;\" data-quarto-disable-processing='true'>\n        <thead>\n        \n              <tr>\n                <th scope=\"col\"> </th>\n                <th scope=\"col\">(1)</th>\n              </tr>\n        </thead>\n        <tfoot><tr><td colspan='2'>+ p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001</td></tr></tfoot>\n        <tbody>\n                <tr>\n                  <td>intercept                        </td>\n                  <td>0.595***</td>\n                </tr>\n                <tr>\n                  <td>                                 </td>\n                  <td>(0.082) </td>\n                </tr>\n                <tr>\n                  <td>news_familypolitically_discordant</td>\n                  <td>0.078*  </td>\n                </tr>\n                <tr>\n                  <td>                                 </td>\n                  <td>(0.039) </td>\n                </tr>\n                <tr>\n                  <td>Num.Obs.                         </td>\n                  <td>86      </td>\n                </tr>\n                <tr>\n                  <td>AIC                              </td>\n                  <td>38.4    </td>\n                </tr>\n                <tr>\n                  <td>BIC                              </td>\n                  <td>48.2    </td>\n                </tr>\n        </tbody>\n      </table>\n    </div>\n<!-- hack to avoid NA insertion in last line -->\n```\n\n:::\n:::\n\n\n\n:::: quiz\n<i class=\"fa fa-question-circle quiz-icon\"></i>\n\n<div>\n\nHow do you interpret this regression table?\n\n</div>\n::::\n\n## Publication bias\n\nTo be done.\n\n## Limits\n\nDo be done. But most importantly, don't compare apples and oranges in your meta-analysis ([see this Data colada post](http://datacolada.org/107#identifier_6_7175)).\n\n## Ressources\n\nI've already mentioned it many times throughout this post, but @harrerDoingMetaAnalysisHandsOn2021 is a really great open-access resource. You can [find the book online here](https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/).\n\n-   Harrer, M., Cuijpers, P., Furukawa, T.A., & Ebert, D.D. (2021). Doing Meta-Analysis with R: A Hands-On Guide. Boca Raton, FL and London: Chapman & Hall/CRC Press. ISBN 978-0-367-61007-4.\n\nAnother recommendation would be [this introductory chapter](https://lakens.github.io/statistical_inferences/) by Daniel Lakens.\n\n-   Lakens, D. (2022). Improving Your Statistical Inferences. Retrieved from https://lakens.github.io/statistical_inferences/. https://doi.org/10.5281/zenodo.6409077\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}